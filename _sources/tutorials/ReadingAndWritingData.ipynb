{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading and Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding dependency \u001b[0m\u001b[1m\u001b[32mio.github.padreati:rapaio-lib:7.0.1\n",
      "\u001b[0mSolving dependencies\n",
      "Resolved artifacts count: 1\n",
      "Add to classpath: \u001b[0m\u001b[32m/home/ati/work/rapaio-jupyter-kernel/target/mima_cache/io/github/padreati/rapaio-lib/7.0.1/rapaio-lib-7.0.1.jar\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%load ../../rapaio-bootstrap.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV - reading and writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and writing CSV files is an important feature for any system which works with data. The reason for its importance is the simplicity of the file format and its popularity.\n",
    "\n",
    "Rapaio library offers support for both reading and writing operations. It has a lot of features and allows flexibility. However we read a file only into a data frame, and we write a csv file only from a data frame. This might look like a constraint in the beginning, but it comes natural since both are tabular data. The only difference is the fact that one operates in the memory of a program and the other one is persisted on disk.\n",
    "\n",
    "### Simple read/write data frames from/into csv files\n",
    "\n",
    "We can read a file with the default options simply by instantiating a `rapaio.io.Csv` object and calls one of `read` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Frame iris = Csv.instance().read(Datasets.class, \"iris-r.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select few rows and inspect what it is inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal-length sepal-width petal-length petal-width   class    \n",
      "[0]     5.1          3.5         1.4          0.2         setosa \n",
      "[1]     4.9          3           1.4          0.2         setosa \n",
      "[2]     7            3.2         4.7          1.4     versicolor \n",
      "[3]     6.4          3.2         4.5          1.5     versicolor \n",
      "[4]     6.3          3.3         6            2.5      virginica \n",
      "[5]     5.8          2.7         5.1          1.9      virginica \n"
     ]
    }
   ],
   "source": [
    "// use only few rows\n",
    "iris.mapRows(0, 1, 50, 51, 100, 101).printContent(textWidth(-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persisting a data frame into csv file format is also simple. We instantiate a `rapaio.io.Csv` object and call one of implementation of `write` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Csv.instance().write(iris, \"/tmp/iris.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open the `/tmp/iris.csv` file with an editor, we can discover that it will have the following content:\n",
    "```\n",
    "sepal-length,sepal-width,petal-length,petal-width,class\n",
    "5.1,3.5,1.4,0.2,setosa\n",
    "4.9,3,1.4,0.2,setosa\n",
    "7,3.2,4.7,1.4,versicolor\n",
    "6.4,3.2,4.5,1.5,versicolor\n",
    "6.3,3.3,6,2.5,virginica\n",
    "5.8,2.7,5.1,1.9,virginica\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various parameters\n",
    "\n",
    "A lof of customization is possible on reading and wrinting csv files. We describe here some of them:\n",
    "\n",
    "* **stripSpaces**: Boolean flag which configures white space trimming for field values. If the white space trimming is enabled, the field values are trimmed at start and end of white char values.\n",
    "* **header**: Boolean flag which if set it considers the first row of the csv file as containing the variable's names. Default value is false.\n",
    "* **quotes**: Boolean flag which specifies if the values are quoted. If eanbled than quote characters are trimmed at read and added at write time. Default values is false.\n",
    "* **separatorChar**: Character used to separate field values.\n",
    "* **escapeChar**: Escape character used. If this feature is turned on, the escape chars are discarded ar read time. This is useful if the separator char is used inside string field values, for example.\n",
    "* **types**: Specific type fields which overrides the automatic type field detection\n",
    "* **naValues**: Values used to identify a missing value placeholders. Default values includes: \"?\", \"\", \" \", \"na\", \"N/A\", \"NaN\"), \"naValues\"\n",
    "* **defaultTypes**: List of automated field types to be tried in the given order during automatic field type detection\n",
    "* **startRow**: Specifies the first row number to be collected from csv file. By default, this value is 0, which means it will collect starting from the first row. If the value is greater than 0 it will skip the first {@code startRow-1} rows.\n",
    "* **endRow**: Specifies the last row number to be collected from csv file. By default, this is value is {@code Integer.MAX_VALUE}, which means all rows from file.\n",
    "* **skipRows**: Skip rows predicate used to filter rows to be read. All row indexes matched by this predicate will not be read.\n",
    "* **skipCols**: Skip columns predicate used to filter columns to be read. All column indexes matched by this predicate will not be read.\n",
    "* **template**: Optional frame templated used to define variable names and type for reading. This overrides auto detection of field names and field types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various read and write methods for Csv\n",
    "\n",
    "Java has a nice abstraction over data named input and output streams.\n",
    "This is enough to make any tool to read or write data from anywhere.\n",
    "We followed that line of thinking by having\n",
    "\n",
    "```java\n",
    "public Frame read(InputStream inputStream) throws IOException\n",
    "public void write(Frame df, OutputStream os) throws IOException\n",
    "```\n",
    "\n",
    "\n",
    "Implemented on Csv class. With these two methods we basically can read from anywhere and can write to anywhere.\n",
    "\n",
    "To simplify some common tasks there are some specialized forms of read and write:\n",
    "\n",
    "* Read from a file giving a `File` instance\n",
    "* Read from a file giving a `String` for path name\n",
    "* Read from a gz archive `File` instance\n",
    "* Read from a resource giving `Class` and `String` for class and name of the resource (this is useful when loading data from a loaded jar or for test)\n",
    "\n",
    "* Write ... \n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java (rjk 2.2.0 preview23)",
   "language": "java",
   "name": "rapaio-jupyter-kernel-preview23"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "java",
   "nbconvert_exporter": "script",
   "pygments_lexer": "java",
   "version": "23.0.1+8-FR"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
